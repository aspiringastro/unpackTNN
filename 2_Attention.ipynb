{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "An attention function can be described as mapping a query and set of key-value pairs to an output, where the query, keys, values and output are all vectors. The output can be computed as a weighted sum of the values, where the weights assigned to each value is computed by the compatibility function of the query with the corresponding key.\n",
    "\n",
    "> How is the output vector $y_i$ produced in self-attention?\n",
    "\n",
    "To produced output vector $y_i$, the self attention operation simply takes a weighted average over all the input vectors.\n",
    "\n",
    "$y_i = \\sum_{j} w_{ij}x_j $\n",
    "\n",
    "where $j$ indexes over the whole sequence and the weights sum to one over all $j$. The weight $w_{ij}$ is not a parameter, as in a normal neural network, but it is dervied from a function over $x_i$ and $x_j$. The simplest option for this function is a dot product:\n",
    "\n",
    "$w'_{ij} = x_i^{T}x_j$\n",
    "\n",
    "Note that $x_i$ is the input vector at the same position as the current vector $y_i$. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The authors call attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension $d_k$ and values of dimension $d_v$ (Although in practice, these dimension are same). We compute the dot productsof the query with all keys, divide each by $\\sqrt(d_k)$, and apply a softmax function to obtain the weights of the values.\n",
    "\n",
    "> What is a good intuition with attention mechanism?\n",
    "\n",
    "Attention mimics thhe retrieval of `value` $v_i$ for a `query` $q$ based on a `key` $k_i$ in database. So the attention can be writing as a probabilistic lookup in the database using the formula:\n",
    "\n",
    "$ Attention(q,k,v) = \\sum_{i}similarity(q,k_i) X v_i $\n",
    "\n",
    "In practice, there are scale normalization and softmax functions applied to make it more effective for stacking\n",
    "\n",
    "$ Attention(Q,K,V) = Softmax(QK^{T}/\\sqrt(d_k))V$\n",
    "\n",
    "> Why is a $ \\sqrt(d_k)$ applied?\n",
    "\n",
    "The $\\sqrt(d_k)$ applied over the dimension $d_k$ is used to bring the scale down to unit variance. This enables the softmax function to work better due to diffusion of the computed values to map the values to $[0,1]$ to ensure that they sum to 1 over the whole sequence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unpackTNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:43:39) [Clang 11.1.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d50dde9429e1a48965dc63651887ff2617ec8ec6c5f3151bc48c4d0688f4d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
