{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "An attention function can be described as mapping a query and set of key-value pairs to an output, where the query, keys, values and output are all vectors. The output can be computed as a weighted sum of the values, where the weights assigned to each value is computed by the compatibility function of the query with the corresponding key.\n",
    "\n",
    "> How is the output vector $y_i$ produced in self-attention?\n",
    "\n",
    "To produced output vector $y_i$, the self attention operation simply takes a weighted average over all the input vectors.\n",
    "\n",
    "$y_i = \\sum_{j} w_{ij}x_j $\n",
    "\n",
    "where $j$ indexes over the whole sequence and the weights sum to one over all $j$. The weight $w_{ij}$ is not a parameter, as in a normal neural network, but it is dervied from a function over $x_i$ and $x_j$. The simplest option for this function is a dot product:\n",
    "\n",
    "$w'_{ij} = x_i^{T}x_j$\n",
    "\n",
    "Note that $x_i$ is the input vector at the same position as the current vector $y_i$. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The authors call attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension $d_k$ and values of dimension $d_v$ (Although in practice, these dimension are same). We compute the dot productsof the query with all keys, divide each by $\\sqrt(d_k)$, and apply a softmax function to obtain the weights of the values.\n",
    "\n",
    "> What is a good intuition with attention mechanism?\n",
    "\n",
    "Attention mimics the retrieval of `value` $v_i$ for a `query` $q$ based on a `key` $k_i$ in database. So the attention can be writing as a probabilistic lookup in the database using the formula:\n",
    "\n",
    "$ Attention(q,k,v) = \\sum_{i}similarity(q,k_i) X v_i $\n",
    "\n",
    "In practice, there are scale normalization and softmax functions applied to make it more effective for stacking\n",
    "\n",
    "$ Attention(Q,K,V) = Softmax(QK^{T}/\\sqrt(d_k))V$\n",
    "\n",
    "\n",
    "> Why is a $ \\sqrt(d_k)$ applied?\n",
    "\n",
    "The $\\sqrt(d_k)$ applied over the dimension $d_k$ is used to bring the scale down to unit variance. This enables the softmax function to work better due to diffusion of the computed values to map the values to $[0,1]$ to ensure that they sum to 1 over the whole sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Embedding(100, 5) Position: Embedding(4, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4]), torch.Size([4, 4, 5]), torch.Size([4, 5]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple character-level tokenization with small code books\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "vocab = string.whitespace + string.ascii_letters + string.digits + string.punctuation\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# create a mapping of vocab to integers\n",
    "vtoi = { ch:i for i,ch in enumerate(vocab)}\n",
    "itov = { i:ch for i,ch in enumerate(vocab)}\n",
    "# encode takes a vocab vector and returns a list of integer tokens\n",
    "encode = lambda v: [vtoi[ch] for ch in v]\n",
    "# decode takes a token vector and returns a list of vocab characters\n",
    "decode = lambda t: [itov[i] for i in t] \n",
    "\n",
    "text = \"Attention is all you need\"\n",
    "text_enc =  encode(text)\n",
    "text_dec = decode(text_enc)\n",
    "data = torch.LongTensor(text_enc)\n",
    "\n",
    "# Let's construct the simplest transformer model meant for visualization only\n",
    "block_size = 4 # maximum content length for prediction\n",
    "n_embd = 5 # number of embedding tokens\n",
    "batch_size = 2 # number of batches processed per pass\n",
    "\n",
    "# We need two embedding tables - one for token with length of vocab_size, and another\n",
    "# for the position embedding with length of block_size. We also need to specify how many\n",
    "# embedding tokens to use for each embedding operation represented as n_embd\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "def get_batch():\n",
    "    ix = torch.randint(0, len(text_enc)-block_size-1, (block_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "x, y = get_batch()\n",
    "print(\"Token:\", token_embedding_table, \"Position:\", position_embedding_table)\n",
    "B, T = x.shape\n",
    "emb_tok = token_embedding_table(x)\n",
    "emb_pos = position_embedding_table(torch.arange(T))\n",
    "x.shape, emb_tok.shape, emb_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: Linear(in_features=5, out_features=6, bias=False) queries: Linear(in_features=5, out_features=6, bias=False)\n",
      "x: torch.Size([4, 4, 5]) key_x: torch.Size([4, 4, 6]) query_x: torch.Size([4, 4, 6])\n",
      "tensor(-0.4661, grad_fn=<MeanBackward0>) tensor(1.3166, grad_fn=<StdBackward0>)\n",
      "tensor(-0.5325, grad_fn=<MeanBackward0>) tensor(1.2047, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# One head of single attention\n",
    "head_size = 6\n",
    "\n",
    "x = emb_pos + emb_tok\n",
    "B,T,C = x.shape\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "print(\"keys:\", key, \"queries:\", query)\n",
    "k = key(x) # B, T, head_size\n",
    "q = query(x) #B, T, head_size\n",
    "print(\"x:\", x.shape, \"key_x:\", k.shape, \"query_x:\", q.shape)\n",
    "W = q @ k.transpose(-2, -1) # transpose the T and head_size so that B,T,h @ B,h,T -> B,T,T\n",
    "W = W * C ** -0.5 # sqrt of C to maintain std and variance\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "W = W.masked_fill(tril == 0, float('-inf'))\n",
    "W = F.softmax(W, dim=-1)\n",
    "v = value(x)\n",
    "out = W @ v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unpackTNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d50dde9429e1a48965dc63651887ff2617ec8ec6c5f3151bc48c4d0688f4d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
